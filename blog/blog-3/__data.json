{"type":"data","nodes":[null,{"type":"data","data":[{"blogNum":1,"markdown":2,"icon":3,"markdownHTML":4,"metadata":5},"3","---\nshortSummary: Ignore the hype – AI still has no moat\nauthor: River / Aditya Shankar\ndated: 2025-05-23\ntitle: Ignore the hype – AI still has no moat\nicon: icon.jpg\n---\n\n## Ignore the hype – AI still has no moat\n\nDespite the massive amount of hype, the larger models – most people are still missing nuance, nearly every single thread of logic via llms points towards AI having no moat\n\nI don't want this to be confused via \"openai has no moat\", since hype/popularity are a moat – if you are the only thing people know, it is likely that people are going to use your platform, but you will still have to constantly advertise or make it convenient to a degree wherein people feel uncomfortable using any other piece of software\n\nGoogle achieved this via ensuring that every other major competetor, firefox or safari – had it's parent owners paid to make google their default search engine, thus creating an illegal monopoly over the search market – and [it looks like this is in a bad state](https://apnews.com/article/google-search-antitrust-case-59114d8bf1dc4c8453c08acaa4051f14)\n\n---\n\n1. \"AGI\" is not the goal, \"Good enough\" is the goal\n\nTake the difference between a (waterman pen)[https://www.waterman.com/] and a regular [0.12€ pen](https://www.promostore.de/nash-kugelschreiber-mit-farbigem-schaft-und-griff-456940.html), a majority of people will not buy a waterman pen – The pens are rediculously expensive, and sell aesthetics and a brand of luxury that most people do not need\n\nFor 90% of use cases, you likely will be able to get away with using a much smaller model, such as [gemma3 27b](https://huggingface.co/google/gemma-3-27b-it) or even much smaller models\n\nThe SOTA model difference between the best open source model (Qwen3 235B w/ 22B active parameters) and the SOTA closed source model is 10% [(65% to 75%)](https://livecodebench.github.io/leaderboard.html) – This is [without considering benchmark overfitting](https://arxiv.org/abs/2503.21934v1)\n\n2. Every single AI tool – has an open source alternative, every. single. one – so programming wise, for a new company to implement these features is not a matter of development complexity but a matter of getting the biggest audience\n\n    Take for example\n\n    - [lifelike text to speech](https://huggingface.co/suno/bark)\n    - [nearly sota coding](https://mistral.ai/news/devstral)\n    - [deep research](https://huggingface.co/blog/open-deep-research)\n    - [text to video](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)\n    - [sota speech to text](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2)\n    - [open source programming editor](https://voideditor.com/)\n\n3. The entire industry is changing at such a significant pace, that nearly nobody is able to keep up and build effective tooling that will last on the longer run\n\n    - A common trope that has come up now is that implementation details are constantly missing features due to the rapid progress of change within the industry\n\n    Changes here are quite evident in the mannerism where\n\n    - The standard for representation of \"how to prompt\" has constantly changed throughout, [older models simply took in a prompt parameter](https://huggingface.co/docs/transformers/main/tasks/prompting), newer models have alternative systems of \"system messages\" and \"user messages\" ([see here](https://huggingface.co/mistralai/Devstral-Small-2505#transformers)) - oh, and system messages themselves in openai [have been renamed to developer messages now](https://lunary.ai/blog/openai-developer-role#what-are-developer-messages)\n\n    - Older tooling frequently used such as Automatic1111 or ComfyUI [have had extensions constantly break](https://github.com/comfyanonymous/ComfyUI/issues/6921) due to the rapid changes occurring in the ecosystem\n    - Newer models now also have been changing, what is considered \"modern\" is hard to track now, is it huggingface, is it comfyui, is it ollama, is it vllm? – what is the best tool to use can be a mind boggling experience\n    - The underlying architecture itself has been significantly changing, leading to a lack of provider support in many circumstances – and it is also now likely going to be upended by [1-bit llms](https://github.com/microsoft/BitNet) or [diffusion models](https://deepmind.google/models/gemini-diffusion/) once again\n\n4. Lastly, from personal experience, for most practical purposes – You will have to likely fine tune a model to the task you are trying to use since most models come with severe lag due to the amount of time it takes to \"load\" the context into a model, which can be heavy if a signficant amount of long context is present/needs to be trained on\n\nThe bottom line here is that, smaller companies or newly emerging big companies in the field still have a very solid start at winning the race","/_app/immutable/assets/icon.BIox6GCx.jpg","\u003Ch2 id=\"ignorethehypeaistillhasnomoat\">Ignore the hype – AI still has no moat\u003C/h2>\n\u003Cp>Despite the massive amount of hype, the larger models – most people are still missing nuance, nearly every single thread of logic via llms points towards AI having no moat\u003C/p>\n\u003Cp>I don't want this to be confused via \"openai has no moat\", since hype/popularity are a moat – if you are the only thing people know, it is likely that people are going to use your platform, but you will still have to constantly advertise or make it convenient to a degree wherein people feel uncomfortable using any other piece of software\u003C/p>\n\u003Cp>Google achieved this via ensuring that every other major competetor, firefox or safari – had it's parent owners paid to make google their default search engine, thus creating an illegal monopoly over the search market – and \u003Ca href=\"https://apnews.com/article/google-search-antitrust-case-59114d8bf1dc4c8453c08acaa4051f14\">it looks like this is in a bad state\u003C/a>\u003C/p>\n\u003Chr />\n\u003Col>\n\u003Cli>\"AGI\" is not the goal, \"Good enough\" is the goal\u003C/li>\n\u003C/ol>\n\u003Cp>Take the difference between a (waterman pen)[https://www.waterman.com/] and a regular \u003Ca href=\"https://www.promostore.de/nash-kugelschreiber-mit-farbigem-schaft-und-griff-456940.html\">0.12€ pen\u003C/a>, a majority of people will not buy a waterman pen – The pens are rediculously expensive, and sell aesthetics and a brand of luxury that most people do not need\u003C/p>\n\u003Cp>For 90% of use cases, you likely will be able to get away with using a much smaller model, such as \u003Ca href=\"https://huggingface.co/google/gemma-3-27b-it\">gemma3 27b\u003C/a> or even much smaller models\u003C/p>\n\u003Cp>The SOTA model difference between the best open source model (Qwen3 235B w/ 22B active parameters) and the SOTA closed source model is 10% \u003Ca href=\"https://livecodebench.github.io/leaderboard.html\">(65% to 75%)\u003C/a> – This is \u003Ca href=\"https://arxiv.org/abs/2503.21934v1\">without considering benchmark overfitting\u003C/a>\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cp>Every single AI tool – has an open source alternative, every. single. one – so programming wise, for a new company to implement these features is not a matter of development complexity but a matter of getting the biggest audience\u003C/p>\n\u003Cp>Take for example\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://huggingface.co/suno/bark\">lifelike text to speech\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://mistral.ai/news/devstral\">nearly sota coding\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://huggingface.co/blog/open-deep-research\">deep research\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://huggingface.co/Wan-AI/Wan2.1-VACE-14B\">text to video\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2\">sota speech to text\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://voideditor.com/\">open source programming editor\u003C/a>\u003C/li>\u003C/ul>\u003C/li>\n\u003Cli>\u003Cp>The entire industry is changing at such a significant pace, that nearly nobody is able to keep up and build effective tooling that will last on the longer run\u003C/p>\n\u003Cul>\n\u003Cli>A common trope that has come up now is that implementation details are constantly missing features due to the rapid progress of change within the industry\u003C/li>\u003C/ul>\n\u003Cp>Changes here are quite evident in the mannerism where\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cp>The standard for representation of \"how to prompt\" has constantly changed throughout, \u003Ca href=\"https://huggingface.co/docs/transformers/main/tasks/prompting\">older models simply took in a prompt parameter\u003C/a>, newer models have alternative systems of \"system messages\" and \"user messages\" (\u003Ca href=\"https://huggingface.co/mistralai/Devstral-Small-2505#transformers\">see here\u003C/a>) - oh, and system messages themselves in openai \u003Ca href=\"https://lunary.ai/blog/openai-developer-role#what-are-developer-messages\">have been renamed to developer messages now\u003C/a>\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Older tooling frequently used such as Automatic1111 or ComfyUI \u003Ca href=\"https://github.com/comfyanonymous/ComfyUI/issues/6921\">have had extensions constantly break\u003C/a> due to the rapid changes occurring in the ecosystem\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>Newer models now also have been changing, what is considered \"modern\" is hard to track now, is it huggingface, is it comfyui, is it ollama, is it vllm? – what is the best tool to use can be a mind boggling experience\u003C/p>\u003C/li>\n\u003Cli>\u003Cp>The underlying architecture itself has been significantly changing, leading to a lack of provider support in many circumstances – and it is also now likely going to be upended by \u003Ca href=\"https://github.com/microsoft/BitNet\">1-bit llms\u003C/a> or \u003Ca href=\"https://deepmind.google/models/gemini-diffusion/\">diffusion models\u003C/a> once again\u003C/p>\u003C/li>\u003C/ul>\u003C/li>\n\u003Cli>\u003Cp>Lastly, from personal experience, for most practical purposes – You will have to likely fine tune a model to the task you are trying to use since most models come with severe lag due to the amount of time it takes to \"load\" the context into a model, which can be heavy if a signficant amount of long context is present/needs to be trained on\u003C/p>\u003C/li>\n\u003C/ol>\n\u003Cp>The bottom line here is that, smaller companies or newly emerging big companies in the field still have a very solid start at winning the race\u003C/p>",{"shortSummary":6,"author":7,"dated":8,"title":6,"icon":9},"Ignore the hype – AI still has no moat","River / Aditya Shankar","2025-05-23","icon.jpg"],"uses":{"params":["num"]}}]}
