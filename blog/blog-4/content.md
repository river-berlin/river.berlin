---
shortSummary: My concerns with the impact of AI on the human psyche
author: River / Aditya Shankar
dated: 2025-05-24
title: My concerns with the impact of AI on the human psyche
icon: icon.jpg
---

(above image by Alexander Wivel from Wikimedia commons, https://en.wikipedia.org/wiki/Brain_in_a_vat#/media/File:Braininvat.jpg, original provided url does return a 404)

## Concerns of the impact of LLMs on the human psyche

I am concerned of how we are not thinking about the psychological ramifications of people using chatgpt as a therapist more widely in tech, I have quite a few friends that are largely using chatgpt as a therapist or a mirror without knowing its ramifications

There are 4 main concerns about this I have

1. ChatGPT relies on flattery to "be more personalized" for users
2. We tend to form emotional bonds with each other, or human-sounding objects
3. AI models hallucinate misinformation
4. Emotional attachment that now advertises sells you products

---

1. ChatGPT relies on flattery to "be more personalized" for users

[This was so bad](https://www.bbc.com/news/articles/cn4jnwdvg9qo) that users saw it agreeing to harm animals over inanimate objects for no logical reason

The underlying reason for this is due to the model being trained on preference modelling via thousands of users – and the common underlying thread was that the model got tuned towards agreeing with people more, as it [performs as a matter of an appeal to emotion](https://en.wikipedia.org/wiki/Appeal_to_flattery)

2. We tend to form emotional bonds with each other, or human-sounding objects

[An ABC investigation](https://www.abc.net.au/news/2025-05-19/young-australians-using-ai-bots-for-therapy/105296348) saw how humans were prone to form emotional bonds with each other

In that a disability support worker, Emma, talks about how she feels emotionally connected to ChatGPT

> She says she likes how the AI bot remembers personal details about her life and incorporates them into their conversations.
  "I used ChatGPT to make a list to pack to move house and I told them that I had a cat.
  "Then when I talked to them about therapy stuff, they're like, 'Oh, you could de-stress by patting your cat,' and it says my cat's name, 'You know, you could pat William and give him scratches or cuddle with him.'

Which sounds really sweet, until you realize point (3) and (4)

3. AI models hallucinate

If you have used chatgpt even a little bit, you will know it is prone to making up links and resources that don't exist - [like inventing fake poetry](https://www.cnbc.com/2022/12/15/google-vs-chatgpt-what-happened-when-i-swapped-services-for-a-day.html)

This sounds harmless, but consider the concern of making up fake psycological conditions or medication, or medication characteristics

4. Imagine something, that you are now emotionally attached to selling you stuff

take the above example for instance

> You know, you could pat William and give him scratches or cuddle with him.

Emma clearly likes that, but what if it also said

> You can also buy william a pack of billiards crispy cat treats, which are running an offer for 2.99€ right now

some LLM vendors [are starting to advertise](https://www.perplexity.ai/hub/blog/why-we-re-experimenting-with-advertising)